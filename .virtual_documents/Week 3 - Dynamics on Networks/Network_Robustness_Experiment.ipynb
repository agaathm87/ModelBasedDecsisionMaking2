


# ---
# Network Robustness Experiment (Full)
# Dynamics *of* networks under random vs targeted node removal
# Includes clustering, path length, and variation analysis
# Author: Michael Lees
# ---

import networkx as nx
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
from tqdm.notebook import tqdm

# Reproducibility
np.random.seed(42)
random.seed(42)






# -----------------------------
# Parameters
# -----------------------------
N = 200
k = 6
p = 0.1
m = 3
p_er = k / (N - 1)

n_trials = 10
fractions = np.linspace(0, 1, 21)





# -----------------------------
# Helper functions
# -----------------------------

def largest_component(G):
    """Return the subgraph of the largest connected component."""
    if len(G) == 0:
        return G
    largest_nodes = max(nx.connected_components(G), key=len)
    return G.subgraph(largest_nodes).copy()

def metrics(G):
    """Compute robustness metrics for the current graph."""
    if len(G) == 0 or G.number_of_edges() == 0:
        return dict(S=0, C=np.nan, L=np.nan)
    GC = largest_component(G)
    S = len(GC) / N
    C = nx.average_clustering(GC)
    try:
        L = nx.average_shortest_path_length(GC)
    except nx.NetworkXError:
        L = np.nan
    return dict(S=S, C=C, L=L)

def remove_nodes(G, f, strategy="random"):
    """Remove a fraction f of nodes using specified strategy."""
    G_copy = G.copy()
    n_remove = int(f * len(G_copy))
    if n_remove == 0:
        return G_copy

    if strategy == "random":
        nodes_to_remove = random.sample(list(G_copy.nodes()), n_remove)
    elif strategy == "targeted":
        degrees = sorted(G_copy.degree, key=lambda x: x[1], reverse=True)
        nodes_to_remove = [n for n, _ in degrees[:n_remove]]
    else:
        raise ValueError("Strategy must be 'random' or 'targeted'")

    G_copy.remove_nodes_from(nodes_to_remove)
    return G_copy



def ws_connected(n, k, beta, max_attempts=40):
    # exact connected WS (retries until connected)
    for _ in range(max_attempts):
        G = nx.watts_strogatz_graph(n, k, beta)
        if nx.is_connected(G):
            return G
    # fallback: largest component
    largest = max(nx.connected_components(G), key=len)
    return G.subgraph(largest).copy()

# or: more connectivity-friendly variant
def nws_connected(n, k, beta):
    # Newman–Watts–Strogatz: keeps ring, adds shortcuts (usually connected)
    G = nx.newman_watts_strogatz_graph(n, k, beta)
    if nx.is_connected(G):
        return G
    largest = max(nx.connected_components(G), key=len)
    return G.subgraph(largest).copy()





# -----------------------------
# Simulation loop
# -----------------------------
results = []

for model_name, generator in [
    ("Erdős–Rényi", lambda: nx.erdos_renyi_graph(N, p_er)),
    ("Watts–Strogatz", lambda: ws_connected(N, k, p)),
    ("Barabási–Albert", lambda: nx.barabasi_albert_graph(N, m))
]:
    for trial in tqdm(range(n_trials), desc=f"Simulating {model_name}"):
        G0 = generator()
        for strategy in ["random", "targeted"]:
            for f in fractions:
                G_removed = remove_nodes(G0, f, strategy)
                vals = metrics(G_removed)
                results.append({
                    "model": model_name,
                    "trial": trial,
                    "strategy": strategy,
                    "fraction_removed": f,
                    **vals
                })

df = pd.DataFrame(results)





# -----------------------------
# Aggregate results
# -----------------------------
summary = (
    df.groupby(["model", "strategy", "fraction_removed"])
    .agg({"S": ["mean", "std"], "C": ["mean", "std"], "L": ["mean", "std"]})
    .reset_index()
)
summary.columns = ["model", "strategy", "fraction_removed",
                   "S_mean", "S_std", "C_mean", "C_std", "L_mean", "L_std"]





# -----------------------------
# Plot 1–3: Mean structural metric
# -----------------------------

fig, axs = plt.subplots(3, 1, figsize=(8, 11), sharex=True)

metrics_labels = [
    ("S_mean", "Relative size of largest component $S$"),
    ("C_mean", "Average clustering coefficient $C$"),
    ("L_mean", "Average path length $L$")
]

colors = {
    "Erdős–Rényi": "#1f77b4",
    "Watts–Strogatz": "#2ca02c",
    "Barabási–Albert": "#9467bd"
}

for ax, (metric, label) in zip(axs, metrics_labels):
    for model in df["model"].unique():
        color = colors[model]
        for strategy, ls, alpha in zip(["random", "targeted"], ["-", "--"], [0.8, 0.8]):
            subset = summary[(summary.model == model) & (summary.strategy == strategy)]
            std_subset = subset
            ax.plot(subset["fraction_removed"], subset[metric],
                    ls=ls, lw=2, color=color,
                    label=f"{model} ({strategy})")
            ax.fill_between(subset["fraction_removed"],
                            subset[metric] - std_subset[f"{metric[0]}_std"],
                            subset[metric] + std_subset[f"{metric[0]}_std"],
                            alpha=0.15, color=color)
    ax.set_xlabel("Fraction of nodes removed")
    ax.set_ylabel(label)
    ax.grid(True, linestyle="--", alpha=0.6)
    ax.set_xlim(0, 1)
    ax.set_ylim(bottom=0)

# --- Overlay legend in the first panel ---
axs[0].legend(
    loc="upper right",
    frameon=True,
    framealpha=0.9,
    fontsize=9,
    ncol=1,
    bbox_to_anchor=(0.98, 0.98)
)

fig.suptitle("Network Robustness (Random vs Targeted)", fontsize=16, y=0.97)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()





# -----------------------------
# Plot 4: Variability across trials (violin plot)
# -----------------------------
plt.figure(figsize=(10, 6))
subset = df[(df["fraction_removed"] == 0.5)]  # mid-removal snapshot
sns.violinplot(
    data=subset, x="model", y="S",
    hue="strategy", split=True, inner="quartile", palette="Set2"
)
plt.title("Variation across trials at 50% node removal")
plt.ylabel("Relative size of largest component $S$")
plt.xlabel("Network model")
plt.grid(True, linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()








# --- Compute Efficiency and Robustness (S=0.5) ---

def find_fraction_for_S(df_model, target=0.5):
    """Find the fraction_removed where S_mean drops to target."""
    x = df_model["fraction_removed"].values
    y = df_model["S_mean"].values
    if np.all(y > target):
        return 1.0
    idx = np.where(y <= target)[0]
    if len(idx) == 0:
        return 1.0
    i = idx[0]
    if i == 0:
        return x[0]
    # Linear interpolation between points (i-1, i)
    x0, x1 = x[i-1], x[i]
    y0, y1 = y[i-1], y[i]
    return x0 + (target - y0) * (x1 - x0) / (y1 - y0)

eff_robust = []

for model in summary["model"].unique():
    # Efficiency: use L_mean at fraction_removed == 0
    L0 = summary[(summary["model"] == model) &
                 (summary["fraction_removed"] == 0)]["L_mean"].mean()
    efficiency = 1 / L0

    for strategy in ["random", "targeted"]:
        df_m = summary[(summary["model"] == model) & (summary["strategy"] == strategy)]
        robustness = find_fraction_for_S(df_m, target=0.5)
        eff_robust.append({
            "model": model,
            "strategy": strategy,
            "efficiency": efficiency,
            "robustness": robustness
        })

df_eff = pd.DataFrame(eff_robust)
display(df_eff)

# --- Plot ---
fig, ax = plt.subplots(figsize=(6,4))
palette = {"Erdős–Rényi": "C0", "Watts–Strogatz": "C1", "Barabási–Albert": "C2"}

for _, row in df_eff.iterrows():
    color = palette[row["model"]]
    marker = 'o' if row["strategy"] == "random" else 'x'
    label = f'{row["model"]} ({row["strategy"]})'
    ax.scatter(row["efficiency"], row["robustness"], color=color, marker=marker, s=80, label=label)

ax.set_xlabel("Efficiency (1 / L(G))", fontsize=11)
ax.set_ylabel("Robustness (fraction removed for S = 0.5)", fontsize=11)
ax.set_title("Network Efficiency vs Robustness", fontsize=12)
ax.legend(fontsize=9)
plt.tight_layout()
plt.show()
