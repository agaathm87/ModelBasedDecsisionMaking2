# ==============================================================================
# ASSIGNMENT 2: DYNAMICS ON NETWORKS - THRESHOLDS AND SPREADING
# Unified Simulation Framework
#
# Course: Model Based Decision-making (5404MBDM6Y)
# Student: 10205071
# Date: November 20, 2025
#
# DESCRIPTION:
# This script integrates graph ingestion (YouTube), Hub-and-Spoke sampling,
# Linear Threshold Model (LTM) simulation with 5 threshold distributions,
# and high-fidelity visualization into a single pipeline.
#
# Dependencies: networkx, matplotlib, pandas, numpy, scipy, gzip
# ==============================================================================

import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import scipy.stats as st
import random
import os
import sys
import time
import gzip
import matplotlib.colors as mcolors
from collections import defaultdict

# ==========================================
# CONFIGURATION & CONSTANTS
# ==========================================

# I/O Settings
INPUT_FILE = "com-youtube.ungraph.txt.gz"
OUTPUT_DIR = "simulation_outputs"
RESULTS_CSV = "monte_carlo_results.csv"
BC_FILE = "sampled_bc_exact.csv"

# Simulation Parameters
N_TARGET_SUBGRAPH = 2000    # Target size for the sampled subgraph
SEED_FRACTION = 0.01        # 1% of nodes to seed active
NUM_SIMULATIONS = 10        # R=10 (Default per assignment hint, validated by stats check)
MAX_STEPS = 50              # Guard clause for convergence

# Statistical Constants for Rigor Check
CONFIDENCE_LEVEL = 0.99 
Z_SCORE = st.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2) 
PRECISION_EPSILON = 0.01 

# Reproducibility
GLOBAL_SEED = 42
random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

# Ensure output directory exists
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================
# MODULE 1: GRAPH INGESTION & SAMPLING
# ==========================================

def load_network_data(filepath):
    """
    Loads the network graph from an edge list file (supports .gz).
    Extracts Largest Connected Component (LCC).
    Falls back to Barabasi-Albert if file missing.
    """
    start_time = time.time()
    G = None
    
    if os.path.exists(filepath):
        print(f"[INFO] Reading edge list from {filepath}...")
        try:
            # Handle GZIP transparency
            if filepath.endswith('.gz'):
                with gzip.open(filepath, 'rt') as f:
                    G = nx.read_edgelist(f, nodetype=int, create_using=nx.Graph)
            else:
                G = nx.read_edgelist(filepath, nodetype=int, create_using=nx.Graph)
        except Exception as e:
            print(f"[WARN] Read failed ({e}). Generating synthetic proxy...")
            G = nx.barabasi_albert_graph(n=5000, m=3, seed=GLOBAL_SEED)
    else:
        print(f"[WARN] File '{filepath}' not found.")
        print(f"[INFO] Generating synthetic Scale-Free proxy...")
        G = nx.barabasi_albert_graph(n=5000, m=3, seed=GLOBAL_SEED)
    
    # Extract LCC
    if not nx.is_connected(G):
        largest_cc = max(nx.connected_components(G), key=len)
        G = G.subgraph(largest_cc).copy()

    elapsed = time.time() - start_time
    print(f"[INFO] LCC Loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")
    print(f"[INFO] Load Time: {elapsed:.4f} seconds")
    return G

def hub_and_spoke_sampling(G_full, N_target=2000):
    """
    Creates a Hub-and-Spoke sample by selecting top hubs and their immediate neighbors.
    This preserves the Scale-Free core-periphery structure better than random sampling.
    """
    if G_full.number_of_nodes() <= N_target:
        return G_full

    print(f"[INFO] Executing Hub-and-Spoke Sampling (Target N={N_target})...")
    
    degree_map = dict(G_full.degree())
    sorted_hubs = sorted(degree_map.keys(), key=degree_map.get, reverse=True)
    
    # Heuristic: Select top 5% as initial hubs
    k_H = max(10, int(len(G_full) * 0.05)) 
    hubs = set(sorted_hubs[:k_H])
    
    # Include Spokes (Neighbors of the Hubs)
    nodes_to_keep = set(hubs)
    for node in hubs:
        nodes_to_keep.update(G_full.neighbors(node))
        if len(nodes_to_keep) >= N_target * 1.5: # Optimization break
            break
            
    # If we have too many, trim by degree; if too few, keep all
    if len(nodes_to_keep) > N_target:
        # Prioritize keeping high degree nodes within the neighborhood
        subgraph_degrees = {n: degree_map[n] for n in nodes_to_keep}
        sorted_selection = sorted(subgraph_degrees, key=subgraph_degrees.get, reverse=True)
        nodes_to_keep = set(sorted_selection[:N_target])
        
    G_sampled = G_full.subgraph(nodes_to_keep).copy()
    
    # Tag nodes for visualization
    nx.set_node_attributes(G_sampled, {n: 'Hub' if n in hubs else 'Spoke' for n in G_sampled.nodes()}, 'role')
    
    print(f"[INFO] Sample Generated: {G_sampled.number_of_nodes()} nodes, {G_sampled.number_of_edges()} edges.")
    return G_sampled

# ==========================================
# MODULE 2: CENTRALITY & SEEDING
# ==========================================

def precompute_centrality(G):
    """
    Calculates Betweenness Centrality on the sampled graph.
    Saves to CSV to demonstrate data persistence.
    Returns the map for simulation usage.
    """
    csv_path = os.path.join(OUTPUT_DIR, BC_FILE)
    print("[INFO] Calculating EXACT Betweenness Centrality (this may take a moment)...")
    start_t = time.time()
    
    bc_map = nx.betweenness_centrality(G, normalized=True, seed=GLOBAL_SEED)
    
    # Save to CSV
    df = pd.DataFrame(bc_map.items(), columns=['Node', 'Betweenness'])
    df = df.sort_values(by='Betweenness', ascending=False)
    df.to_csv(csv_path, index=False)
    
    print(f"[INFO] Centrality calculated in {time.time()-start_t:.2f}s. Saved to {csv_path}")
    return bc_map

def get_seeds(G, strategy, fraction, bc_map=None):
    """Selects seeds based on the specified strategy."""
    N = G.number_of_nodes()
    num_seeds = max(1, int(N * fraction))
    
    nodes = list(G.nodes())
    
    if strategy == 'Random':
        return random.sample(nodes, num_seeds)
    
    elif strategy == 'Degree':
        # Recalculate degree for the specific subgraph
        degree_map = dict(G.degree())
        sorted_nodes = sorted(degree_map.keys(), key=degree_map.get, reverse=True)
        return sorted_nodes[:num_seeds]
        
    elif strategy == 'Betweenness':
        if bc_map is None:
            # Fallback if not precomputed
            bc_map = nx.betweenness_centrality(G, normalized=True)
        # Sort using the precomputed map
        sorted_nodes = sorted(bc_map.keys(), key=lambda x: bc_map.get(x, 0), reverse=True)
        return sorted_nodes[:num_seeds]
        
    return []

# ==========================================
# MODULE 3: THRESHOLD DYNAMICS (LTM)
# ==========================================

def get_thresholds(G, scenario_key):
    """
    Generates threshold values based on 5 distinct distributions.
    """
    N = len(G)
    
    # 1. Homogeneous
    if scenario_key == 'Fixed_0.2':
        return {n: 0.2 for n in G.nodes()}
        
    # 2. Maximal Heterogeneity
    elif scenario_key == 'Uniform':
        return {n: np.random.uniform(0, 1) for n in G.nodes()}
        
    # 3. Beta Scenarios (Skewed)
    elif scenario_key == 'Beta_Low': # Receptive (Mean ~0.29)
        return {n: np.random.beta(2, 5) for n in G.nodes()}
        
    elif scenario_key == 'Beta_Mid': # Symmetric (Mean 0.5)
        return {n: np.random.beta(2, 2) for n in G.nodes()}
        
    elif scenario_key == 'Beta_High': # Resistant (Mean ~0.71)
        return {n: np.random.beta(5, 2) for n in G.nodes()}
        
    return {n: 0.2 for n in G.nodes()}

def run_threshold_simulation(G, seeds, thresholds):
    """
    Executes one run of the Linear Threshold Model.
    Returns adoption history and final state stats.
    """
    # Initialization
    active_set = set(seeds)
    history = [len(active_set) / len(G)]
    
    # Pre-compute neighbors to speed up inner loop
    adj = {n: set(G.neighbors(n)) for n in G.nodes()}
    degrees = {n: len(adj[n]) for n in G.nodes()}
    
    for step in range(MAX_STEPS):
        newly_active = set()
        
        # Optimization: Check only inactive nodes connected to active ones
        # For density, iterating all inactive is safer for correctness
        inactive_nodes = [n for n in G.nodes() if n not in active_set]
        
        if not inactive_nodes:
            break
            
        for node in inactive_nodes:
            deg = degrees[node]
            if deg == 0: continue
            
            # Count active neighbors
            active_neighbors = len([nbr for nbr in adj[node] if nbr in active_set])
            influence = active_neighbors / deg
            
            if influence >= thresholds[node]:
                newly_active.add(node)
        
        if newly_active:
            active_set.update(newly_active)
            history.append(len(active_set) / len(G))
        else:
            # Convergence reached
            history.extend([history[-1]] * (MAX_STEPS - step - 1))
            break
            
    return history, len(active_set)

# ==========================================
# MODULE 4: EXECUTION & STATS
# ==========================================

def calculate_statistical_rigor(pilot_variances):
    """
    Calculates R_req for statistical rigor based on pilot data variance.
    Formula: R_req >= (Z * s0 / epsilon)^2
    """
    print("\n" + "="*40)
    print("STATISTICAL RIGOR CHECK")
    print("="*40)
    
    max_r_req = 0
    
    for scenario, s0 in pilot_variances.items():
        if s0 == 0: continue
        r_req = (Z_SCORE * s0 / PRECISION_EPSILON)**2
        print(f"Scenario [{scenario}]: StdDev={s0:.4f} -> R_req >= {r_req:.1f}")
        max_r_req = max(max_r_req, r_req)
        
    print(f"\n[DECISION] Recommended Runs for {CONFIDENCE_LEVEL*100}% Confidence: {int(np.ceil(max_r_req))}")
    print(f"[ACTION] Using configured NUM_SIMULATIONS = {NUM_SIMULATIONS}")
    return max_r_req

def execute_experiment_suite(G, bc_map):
    """
    Runs all combinations of Strategies x Threshold Distributions.
    """
    strategies = ['Random', 'Degree', 'Betweenness']
    threshold_scenarios = ['Fixed_0.2', 'Uniform', 'Beta_Low', 'Beta_Mid', 'Beta_High']
    
    results_data = []
    pilot_std_tracker = defaultdict(list) # To track final sizes for variance calc
    
    print(f"\n[INFO] Starting Experiment Suite (R={NUM_SIMULATIONS} per setting)...")
    
    total_iter = len(strategies) * len(threshold_scenarios) * NUM_SIMULATIONS
    pbar = 0
    
    for strat in strategies:
        for thresh_key in threshold_scenarios:
            
            # Pre-draw seeds if deterministic strategy (optimization)
            # However, to capture variance in 'Random' strategy, we redraw inside loop
            
            for run_id in range(NUM_SIMULATIONS):
                # Set run-specific seed
                run_seed = GLOBAL_SEED + (pbar * 100) + run_id
                random.seed(run_seed)
                np.random.seed(run_seed)
                
                # Get Seeds
                seeds = get_seeds(G, strat, SEED_FRACTION, bc_map)
                
                # Get Thresholds
                thresholds = get_thresholds(G, thresh_key)
                
                # Run Sim
                curve, final_count = run_threshold_simulation(G, seeds, thresholds)
                
                # Store Data
                key = f"{strat}_{thresh_key}"
                pilot_std_tracker[key].append(curve[-1])
                
                # Record Time Series (Unrolling for CSV)
                for t, val in enumerate(curve):
                    results_data.append({
                        'RunID': run_id,
                        'Strategy': strat,
                        'Threshold_Dist': thresh_key,
                        'Step': t,
                        'Adoption_Fraction': val
                    })
                
                pbar += 1
                if pbar % 50 == 0:
                    print(f" -> Completed {pbar}/{total_iter} simulations...")

    # Post-Simulation: Calculate Standard Deviations for Rigor Check
    variances = {k: np.std(v) for k, v in pilot_std_tracker.items()}
    calculate_statistical_rigor(variances)
    
    return pd.DataFrame(results_data)

# ==========================================
# MODULE 5: VISUALIZATION
# ==========================================

def visualize_network_structure(G):
    """
    Visualizes the Hub-and-Spoke topology.
    Nodes colored by Role (Hub/Spoke), sized by Degree.
    """
    if len(G) > 3000:
        print("[WARN] Graph too large to plot cleanly. Skipping.")
        return

    print("\n[INFO] Generating Network Topology Plot...")
    plt.figure(figsize=(12, 12))
    
    pos = nx.spring_layout(G, k=0.10, seed=GLOBAL_SEED)
    
    # Attributes
    roles = nx.get_node_attributes(G, 'role')
    degrees = dict(G.degree())
    
    # Separate lists
    hubs = [n for n, r in roles.items() if r == 'Hub']
    spokes = [n for n, r in roles.items() if r != 'Hub']
    
    # Draw Spokes
    nx.draw_networkx_nodes(G, pos, nodelist=spokes, node_size=15, 
                          node_color='#4682B4', alpha=0.4, label='Spokes')
    
    # Draw Hubs (Sized by degree)
    hub_sizes = [degrees[n] * 3 for n in hubs]
    nx.draw_networkx_nodes(G, pos, nodelist=hubs, node_size=hub_sizes, 
                          node_color='#DC143C', alpha=0.9, label='Hubs')
    
    nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray')
    
    plt.title(f"YouTube Hub-and-Spoke Sample (N={len(G)})", fontsize=15)
    plt.legend(loc='upper right')
    plt.axis('off')
    
    out_path = os.path.join(OUTPUT_DIR, "network_topology.png")
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[INFO] Saved topology plot to {out_path}")

def plot_comparative_curves(df):
    """
    Generates a multipanel plot comparing strategies across the 5 distributions.
    """
    print("[INFO] Generating Comparative Adoption Curves...")
    
    scenarios = df['Threshold_Dist'].unique()
    strategies = df['Strategy'].unique()
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12), sharey=True)
    axes = axes.flatten()
    
    # Colors and Styles
    styles = {'Random': ':', 'Degree': '--', 'Betweenness': '-'}
    palette = sns.color_palette("tab10", 3) if 'seaborn' in sys.modules else ['b', 'g', 'r']
    
    # Aggregate Mean Curves
    mean_df = df.groupby(['Strategy', 'Threshold_Dist', 'Step'])['Adoption_Fraction'].mean().reset_index()
    
    for idx, scenario in enumerate(scenarios):
        ax = axes[idx]
        subset = mean_df[mean_df['Threshold_Dist'] == scenario]
        
        for i, strat in enumerate(strategies):
            strat_data = subset[subset['Strategy'] == strat]
            ax.plot(strat_data['Step'], strat_data['Adoption_Fraction'], 
                   label=strat, linestyle=styles.get(strat, '-'), linewidth=2)
            
        ax.set_title(f"Scenario: {scenario}", fontsize=12, fontweight='bold')
        ax.set_xlabel("Time Step")
        if idx % 3 == 0:
            ax.set_ylabel("Adoption Fraction")
        ax.grid(True, alpha=0.3)
        if idx == 0:
            ax.legend(title="Seeding Strategy")
            
    # Hide unused subplot if 5 scenarios
    if len(scenarios) < 6:
        axes[-1].axis('off')
        
    plt.suptitle("Dynamics of Contagion: Strategy vs. Threshold Heterogeneity", fontsize=16, y=0.98)
    plt.tight_layout()
    
    out_path = os.path.join(OUTPUT_DIR, "adoption_curves_comparison.png")
    plt.savefig(out_path, dpi=300)
    plt.show()
    print(f"[INFO] Saved curves to {out_path}")

# ==========================================
# MAIN CONTROLLER
# ==========================================

def main():
    print("=== YouTube Threshold Dynamics Simulation Framework ===")
    
    # 1. Load & Sample
    G_raw = load_network_data(INPUT_FILE)
    G_sample = hub_and_spoke_sampling(G_raw, N_TARGET_SUBGRAPH)
    
    # 2. Pre-compute Centrality (Optimization)
    bc_map = precompute_centrality(G_sample)
    
    # 3. Visualize Topology
    visualize_network_structure(G_sample)
    
    # 4. Execute Simulations
    df_results = execute_experiment_suite(G_sample, bc_map)
    
    # 5. Save Raw Data
    csv_path = os.path.join(OUTPUT_DIR, RESULTS_CSV)
    df_results.to_csv(csv_path, index=False)
    print(f"[INFO] Raw data saved to {csv_path}")
    
    # 6. Visualize Results
    plot_comparative_curves(df_results)
    
    print("\n[INFO] Pipeline Complete. Check 'simulation_outputs' folder.")

if __name__ == "__main__":
    main()



