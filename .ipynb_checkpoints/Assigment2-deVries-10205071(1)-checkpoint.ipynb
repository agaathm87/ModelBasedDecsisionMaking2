{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 2: DYNAMICS ON NETWORKS - THRESHOLDS AND SPREADING\n",
    "## Unified Simulation Framework\n",
    "\n",
    "**Course:** Model Based Decision-making (5404MBDM6Y)  \n",
    "**Student:** 10205071  \n",
    "**Date:** November 20, 2025\n",
    "\n",
    "### Description\n",
    "This notebook integrates graph ingestion (YouTube), Hub-and-Spoke sampling, Linear Threshold Model (LTM) simulation with 5 threshold distributions, and high-fidelity visualization into a single pipeline.\n",
    "\n",
    "**Dependencies:** `networkx`, `matplotlib`, `pandas`, `numpy`, `scipy`, `gzip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION & CONSTANTS\n",
    "# ==========================================\n",
    "\n",
    "# I/O Settings\n",
    "INPUT_FILE = \"com-youtube.ungraph.txt.gz\"\n",
    "OUTPUT_DIR = \"simulation_outputs\"\n",
    "RESULTS_CSV = \"monte_carlo_results.csv\"\n",
    "BC_FILE = \"sampled_bc_exact.csv\"\n",
    "\n",
    "# Simulation Parameters\n",
    "N_TARGET_SUBGRAPH = 2000    # Target size for the sampled subgraph\n",
    "SEED_FRACTION = 0.01        # 1% of nodes to seed active\n",
    "NUM_SIMULATIONS = 10        # R=10 (Default per assignment hint, validated by stats check)\n",
    "MAX_STEPS = 50              # Guard clause for convergence\n",
    "\n",
    "# Statistical Constants for Rigor Check\n",
    "CONFIDENCE_LEVEL = 0.99 \n",
    "Z_SCORE = st.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2) \n",
    "PRECISION_EPSILON = 0.01 \n",
    "\n",
    "# Reproducibility\n",
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Graph Ingestion & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the network graph from an edge list file (supports .gz).\n",
    "    Extracts Largest Connected Component (LCC).\n",
    "    Falls back to Barabasi-Albert if file missing.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    G = None\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"[INFO] Reading edge list from {filepath}...\")\n",
    "        try:\n",
    "            # Handle GZIP transparency\n",
    "            if filepath.endswith('.gz'):\n",
    "                with gzip.open(filepath, 'rt') as f:\n",
    "                    G = nx.read_edgelist(f, nodetype=int, create_using=nx.Graph)\n",
    "            else:\n",
    "                G = nx.read_edgelist(filepath, nodetype=int, create_using=nx.Graph)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Read failed ({e}). Generating synthetic proxy...\")\n",
    "            G = nx.barabasi_albert_graph(n=5000, m=3, seed=GLOBAL_SEED)\n",
    "    else:\n",
    "        print(f\"[WARN] File '{filepath}' not found.\")\n",
    "        print(f\"[INFO] Generating synthetic Scale-Free proxy...\")\n",
    "        G = nx.barabasi_albert_graph(n=5000, m=3, seed=GLOBAL_SEED)\n",
    "    \n",
    "    # Extract LCC\n",
    "    if not nx.is_connected(G):\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[INFO] LCC Loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    print(f\"[INFO] Load Time: {elapsed:.4f} seconds\")\n",
    "    return G\n",
    "\n",
    "def hub_and_spoke_sampling(G_full, N_target=2000):\n",
    "    \"\"\"\n",
    "    Creates a Hub-and-Spoke sample by selecting top hubs and their immediate neighbors.\n",
    "    This preserves the Scale-Free core-periphery structure better than random sampling.\n",
    "    \"\"\"\n",
    "    if G_full.number_of_nodes() <= N_target:\n",
    "        return G_full\n",
    "\n",
    "    print(f\"[INFO] Executing Hub-and-Spoke Sampling (Target N={N_target})...\")\n",
    "    \n",
    "    degree_map = dict(G_full.degree())\n",
    "    sorted_hubs = sorted(degree_map.keys(), key=degree_map.get, reverse=True)\n",
    "    \n",
    "    # Heuristic: Select top 5% as initial hubs\n",
    "    k_H = max(10, int(len(G_full) * 0.05)) \n",
    "    hubs = set(sorted_hubs[:k_H])\n",
    "    \n",
    "    # Include Spokes (Neighbors of the Hubs)\n",
    "    nodes_to_keep = set(hubs)\n",
    "    for node in hubs:\n",
    "        nodes_to_keep.update(G_full.neighbors(node))\n",
    "        if len(nodes_to_keep) >= N_target * 1.5: # Optimization break\n",
    "            break\n",
    "            \n",
    "    # If we have too many, trim by degree; if too few, keep all\n",
    "    if len(nodes_to_keep) > N_target:\n",
    "        # Prioritize keeping high degree nodes within the neighborhood\n",
    "        subgraph_degrees = {n: degree_map[n] for n in nodes_to_keep}\n",
    "        sorted_selection = sorted(subgraph_degrees, key=subgraph_degrees.get, reverse=True)\n",
    "        nodes_to_keep = set(sorted_selection[:N_target])\n",
    "        \n",
    "    G_sampled = G_full.subgraph(nodes_to_keep).copy()\n",
    "    \n",
    "    # Tag nodes for visualization\n",
    "    nx.set_node_attributes(G_sampled, {n: 'Hub' if n in hubs else 'Spoke' for n in G_sampled.nodes()}, 'role')\n",
    "    \n",
    "    print(f\"[INFO] Sample Generated: {G_sampled.number_of_nodes()} nodes, {G_sampled.number_of_edges()} edges.\")\n",
    "    return G_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Centrality & Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_centrality(G):\n",
    "    \"\"\"\n",
    "    Calculates Betweenness Centrality on the sampled graph.\n",
    "    Saves to CSV to demonstrate data persistence.\n",
    "    Returns the map for simulation usage.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, BC_FILE)\n",
    "    print(\"[INFO] Calculating EXACT Betweenness Centrality (this may take a moment)...\")\n",
    "    start_t = time.time()\n",
    "    \n",
    "    bc_map = nx.betweenness_centrality(G, normalized=True, seed=GLOBAL_SEED)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(bc_map.items(), columns=['Node', 'Betweenness'])\n",
    "    df = df.sort_values(by='Betweenness', ascending=False)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"[INFO] Centrality calculated in {time.time()-start_t:.2f}s. Saved to {csv_path}\")\n",
    "    return bc_map\n",
    "\n",
    "def get_seeds(G, strategy, fraction, bc_map=None):\n",
    "    \"\"\"Selects seeds based on the specified strategy.\"\"\"\n",
    "    N = G.number_of_nodes()\n",
    "    num_seeds = max(1, int(N * fraction))\n",
    "    \n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    if strategy == 'Random':\n",
    "        return random.sample(nodes, num_seeds)\n",
    "    \n",
    "    elif strategy == 'Degree':\n",
    "        # Recalculate degree for the specific subgraph\n",
    "        degree_map = dict(G.degree())\n",
    "        sorted_nodes = sorted(degree_map.keys(), key=degree_map.get, reverse=True)\n",
    "        return sorted_nodes[:num_seeds]\n",
    "        \n",
    "    elif strategy == 'Betweenness':\n",
    "        if bc_map is None:\n",
    "            # Fallback if not precomputed\n",
    "            bc_map = nx.betweenness_centrality(G, normalized=True)\n",
    "        # Sort using the precomputed map\n",
    "        sorted_nodes = sorted(bc_map.keys(), key=lambda x: bc_map.get(x, 0), reverse=True)\n",
    "        return sorted_nodes[:num_seeds]\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Threshold Dynamics (LTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds(G, scenario_key):\n",
    "    \"\"\"\n",
    "    Generates threshold values based on 5 distinct distributions.\n",
    "    \"\"\"\n",
    "    N = len(G)\n",
    "    \n",
    "    # 1. Homogeneous\n",
    "    if scenario_key == 'Fixed_0.2':\n",
    "        return {n: 0.2 for n in G.nodes()}\n",
    "        \n",
    "    # 2. Maximal Heterogeneity\n",
    "    elif scenario_key == 'Uniform':\n",
    "        return {n: np.random.uniform(0, 1) for n in G.nodes()}\n",
    "        \n",
    "    # 3. Beta Scenarios (Skewed)\n",
    "    elif scenario_key == 'Beta_Low': # Receptive (Mean ~0.29)\n",
    "        return {n: np.random.beta(2, 5) for n in G.nodes()}\n",
    "        \n",
    "    elif scenario_key == 'Beta_Mid': # Symmetric (Mean 0.5)\n",
    "        return {n: np.random.beta(2, 2) for n in G.nodes()}\n",
    "        \n",
    "    elif scenario_key == 'Beta_High': # Resistant (Mean ~0.71)\n",
    "        return {n: np.random.beta(5, 2) for n in G.nodes()}\n",
    "        \n",
    "    return {n: 0.2 for n in G.nodes()}\n",
    "\n",
    "def run_threshold_simulation(G, seeds, thresholds):\n",
    "    \"\"\"\n",
    "    Executes one run of the Linear Threshold Model.\n",
    "    Returns adoption history and final state stats.\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    active_set = set(seeds)\n",
    "    history = [len(active_set) / len(G)]\n",
    "    \n",
    "    # Pre-compute neighbors to speed up inner loop\n",
    "    adj = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
    "    degrees = {n: len(adj[n]) for n in G.nodes()}\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        newly_active = set()\n",
    "        \n",
    "        # Optimization: Check only inactive nodes connected to active ones\n",
    "        # For density, iterating all inactive is safer for correctness\n",
    "        inactive_nodes = [n for n in G.nodes() if n not in active_set]\n",
    "        \n",
    "        if not inactive_nodes:\n",
    "            break\n",
    "            \n",
    "        for node in inactive_nodes:\n",
    "            deg = degrees[node]\n",
    "            if deg == 0: continue\n",
    "            \n",
    "            # Count active neighbors\n",
    "            active_neighbors = len([nbr for nbr in adj[node] if nbr in active_set])\n",
    "            influence = active_neighbors / deg\n",
    "            \n",
    "            if influence >= thresholds[node]:\n",
    "                newly_active.add(node)\n",
    "        \n",
    "        if newly_active:\n",
    "            active_set.update(newly_active)\n",
    "            history.append(len(active_set) / len(G))\n",
    "        else:\n",
    "            # Convergence reached\n",
    "            history.extend([history[-1]] * (MAX_STEPS - step - 1))\n",
    "            break\n",
    "            \n",
    "    return history, len(active_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Execution & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistical_rigor(pilot_variances):\n",
    "    \"\"\"\n",
    "    Calculates R_req for statistical rigor based on pilot data variance.\n",
    "    Formula: R_req >= (Z * s0 / epsilon)^2\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"STATISTICAL RIGOR CHECK\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    max_r_req = 0\n",
    "    \n",
    "    for scenario, s0 in pilot_variances.items():\n",
    "        if s0 == 0: continue\n",
    "        r_req = (Z_SCORE * s0 / PRECISION_EPSILON)**2\n",
    "        print(f\"Scenario [{scenario}]: StdDev={s0:.4f} -> R_req >= {r_req:.1f}\")\n",
    "        max_r_req = max(max_r_req, r_req)\n",
    "        \n",
    "    print(f\"\\n[DECISION] Recommended Runs for {CONFIDENCE_LEVEL*100}% Confidence: {int(np.ceil(max_r_req))}\")\n",
    "    print(f\"[ACTION] Using configured NUM_SIMULATIONS = {NUM_SIMULATIONS}\")\n",
    "    return max_r_req\n",
    "\n",
    "def execute_experiment_suite(G, bc_map):\n",
    "    \"\"\"\n",
    "    Runs all combinations of Strategies x Threshold Distributions.\n",
    "    \"\"\"\n",
    "    strategies = ['Random', 'Degree', 'Betweenness']\n",
    "    threshold_scenarios = ['Fixed_0.2', 'Uniform', 'Beta_Low', 'Beta_Mid', 'Beta_High']\n",
    "    \n",
    "    results_data = []\n",
    "    pilot_std_tracker = defaultdict(list) # To track final sizes for variance calc\n",
    "    \n",
    "    print(f\"\\n[INFO] Starting Experiment Suite (R={NUM_SIMULATIONS} per setting)...\")\n",
    "    \n",
    "    total_iter = len(strategies) * len(threshold_scenarios) * NUM_SIMULATIONS\n",
    "    pbar = 0\n",
    "    \n",
    "    for strat in strategies:\n",
    "        for thresh_key in threshold_scenarios:\n",
    "            \n",
    "            # Pre-draw seeds if deterministic strategy (optimization)\n",
    "            # However, to capture variance in 'Random' strategy, we redraw inside loop\n",
    "            \n",
    "            for run_id in range(NUM_SIMULATIONS):\n",
    "                # Set run-specific seed\n",
    "                run_seed = GLOBAL_SEED + (pbar * 100) + run_id\n",
    "                random.seed(run_seed)\n",
    "                np.random.seed(run_seed)\n",
    "                \n",
    "                # Get Seeds\n",
    "                seeds = get_seeds(G, strat, SEED_FRACTION, bc_map)\n",
    "                \n",
    "                # Get Thresholds\n",
    "                thresholds = get_thresholds(G, thresh_key)\n",
    "                \n",
    "                # Run Sim\n",
    "                curve, final_count = run_threshold_simulation(G, seeds, thresholds)\n",
    "                \n",
    "                # Store Data\n",
    "                key = f\"{strat}_{thresh_key}\"\n",
    "                pilot_std_tracker[key].append(curve[-1])\n",
    "                \n",
    "                # Record Time Series (Unrolling for CSV)\n",
    "                for t, val in enumerate(curve):\n",
    "                    results_data.append({\n",
    "                        'RunID': run_id,\n",
    "                        'Strategy': strat,\n",
    "                        'Threshold_Dist': thresh_key,\n",
    "                        'Step': t,\n",
    "                        'Adoption_Fraction': val\n",
    "                    })\n",
    "                \n",
    "                pbar += 1\n",
    "                if pbar % 50 == 0:\n",
    "                    print(f\" -> Completed {pbar}/{total_iter} simulations...\")\n",
    "\n",
    "    # Post-Simulation: Calculate Standard Deviations for Rigor Check\n",
    "    variances = {k: np.std(v) for k, v in pilot_std_tracker.items()}\n",
    "    calculate_statistical_rigor(variances)\n",
    "    \n",
    "    return pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_structure(G):\n",
    "    \"\"\"\n",
    "    Visualizes the Hub-and-Spoke topology.\n",
    "    Nodes colored by Role (Hub/Spoke), sized by Degree.\n",
    "    \"\"\"\n",
    "    if len(G) > 3000:\n",
    "        print(\"[WARN] Graph too large to plot cleanly. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n[INFO] Generating Network Topology Plot...\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    pos = nx.spring_layout(G, k=0.10, seed=GLOBAL_SEED)\n",
    "    \n",
    "    # Attributes\n",
    "    roles = nx.get_node_attributes(G, 'role')\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    # Separate lists\n",
    "    hubs = [n for n, r in roles.items() if r == 'Hub']\n",
    "    spokes = [n for n, r in roles.items() if r != 'Hub']\n",
    "    \n",
    "    # Draw Spokes\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=spokes, node_size=15, \n",
    "                          node_color='#4682B4', alpha=0.4, label='Spokes')\n",
    "    \n",
    "    # Draw Hubs (Sized by degree)\n",
    "    hub_sizes = [degrees[n] * 3 for n in hubs]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=hubs, node_size=hub_sizes, \n",
    "                          node_color='#DC143C', alpha=0.9, label='Hubs')\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray')\n",
    "    \n",
    "    plt.title(f\"YouTube Hub-and-Spoke Sample (N={len(G)})\", fontsize=15)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, \"network_topology.png\")\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved topology plot to {out_path}\")\n",
    "\n",
    "def plot_comparative_curves(df):\n",
    "    \"\"\"\n",
    "    Generates a multipanel plot comparing strategies across the 5 distributions.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Generating Comparative Adoption Curves...\")\n",
    "    \n",
    "    scenarios = df['Threshold_Dist'].unique()\n",
    "    strategies = df['Strategy'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12), sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Colors and Styles\n",
    "    styles = {'Random': ':', 'Degree': '--', 'Betweenness': '-'}\n",
    "    palette = sns.color_palette(\"tab10\", 3) if 'seaborn' in sys.modules else ['b', 'g', 'r']\n",
    "    \n",
    "    # Aggregate Mean Curves\n",
    "    mean_df = df.groupby(['Strategy', 'Threshold_Dist', 'Step'])['Adoption_Fraction'].mean().reset_index()\n",
    "    \n",
    "    for idx, scenario in enumerate(scenarios):\n",
    "        ax = axes[idx]\n",
    "        subset = mean_df[mean_df['Threshold_Dist'] == scenario]\n",
    "        \n",
    "        for i, strat in enumerate(strategies):\n",
    "            strat_data = subset[subset['Strategy'] == strat]\n",
    "            ax.plot(strat_data['Step'], strat_data['Adoption_Fraction'], \n",
    "                   label=strat, linestyle=styles.get(strat, '-'), linewidth=2)\n",
    "            \n",
    "        ax.set_title(f\"Scenario: {scenario}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(\"Time Step\")\n",
    "        if idx % 3 == 0:\n",
    "            ax.set_ylabel(\"Adoption Fraction\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            ax.legend(title=\"Seeding Strategy\")\n",
    "            \n",
    "    # Hide unused subplot if 5 scenarios\n",
    "    if len(scenarios) < 6:\n",
    "        axes[-1].axis('off')\n",
    "        \n",
    "    plt.suptitle(\"Dynamics of Contagion: Strategy vs. Threshold Heterogeneity\", fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, \"adoption_curves_comparison.png\")\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"[INFO] Saved curves to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load & Sample\n",
    "G_raw = load_network_data(INPUT_FILE)\n",
    "G_sample = hub_and_spoke_sampling(G_raw, N_TARGET_SUBGRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pre-compute Centrality (Optimization)\n",
    "bc_map = precompute_centrality(G_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize Topology\n",
    "visualize_network_structure(G_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Execute Simulations\n",
    "df_results = execute_experiment_suite(G_sample, bc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Raw Data\n",
    "csv_path = os.path.join(OUTPUT_DIR, RESULTS_CSV)\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"[INFO] Raw data saved to {csv_path}\")\n",
    "\n",
    "# 6. Visualize Results\n",
    "plot_comparative_curves(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}